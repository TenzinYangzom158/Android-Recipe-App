{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Lab8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMKWEheX1se9NdDY3xNKj4q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TenzinYangzom158/Android-Recipe-App/blob/main/NLP_Lab8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Program 8 : Write a program to differentiate stemming and lemmatizing words"
      ],
      "metadata": {
        "id": "-zmlCKQaHSZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVyiYtbxIP3N",
        "outputId": "1f12d0f2-8a1d-498a-9b2d-22899bdd5829"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming\n",
        "Stemming is a method of normalization of words in Natural Language Processing. It is a technique in which a set of words in a sentence are converted into a sequence to shorten its lookup. In this method, the words having the same meaning but have some variations according to the context or sentence are normalized."
      ],
      "metadata": {
        "id": "yCyB7YFsJyN3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4RMIaHxGWk0",
        "outputId": "e7c15597-cdc8-47e9-96c4-b8e02e1f8c73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wait : wait\n",
            "waiting : wait\n",
            "waited : wait\n",
            "waits : wait\n"
          ]
        }
      ],
      "source": [
        "# stemming on words in the list\n",
        "from nltk.stem import PorterStemmer\n",
        "words= [\"wait\", \"waiting\", \"waited\", \"waits\"]\n",
        "ps =PorterStemmer()\n",
        "for w in words:\n",
        "    rootWord=ps.stem(w)\n",
        "    print(w, \":\",rootWord)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stemming on the sentece\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "sentence=\"Write a program to differentiate stemming and lemmatizing words\"\n",
        "words = word_tokenize(sentence)\n",
        "ps = PorterStemmer()\n",
        "print(\"Stemming for\")\n",
        "print(\"-------------\")\n",
        "for w in words:\n",
        "\trootWord=ps.stem(w)\n",
        "\tprint(w+\":\",rootWord)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyVsPt0UI1hG",
        "outputId": "5bb6a3f7-1fbc-48ac-c4b0-0ac96cdcdc5a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming for\n",
            "-------------\n",
            "Write: write\n",
            "a: a\n",
            "program: program\n",
            "to: to\n",
            "differentiate: differenti\n",
            "stemming: stem\n",
            "and: and\n",
            "lemmatizing: lemmat\n",
            "words: word\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code to distinguish between Lemmatization and Stemming"
      ],
      "metadata": {
        "id": "qAq8kMksKXFg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization\n",
        "Lemmatization in NLTK is the algorithmic process of finding the lemma of a word depending on its meaning and context. Lemmatization usually refers to the morphological analysis of words, which aims to remove inflectional endings. It helps in returning the base or dictionary form of a word known as the lemma."
      ],
      "metadata": {
        "id": "FeowHvgHJ_mC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "text = \"Write a program to differentiate stemming and lemmatizing words\"\n",
        "tokenization = nltk.word_tokenize(text)\n",
        "print(\"Lemma for\")\n",
        "print(\"-------------\")\n",
        "for w in tokenization:\n",
        "  print(\"{} : {}\".format(w, wordnet_lemmatizer.lemmatize(w)))  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdudoBgpJilu",
        "outputId": "211cbfb1-05bb-47d7-894b-8bd53e9a4fed"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemma for\n",
            "-------------\n",
            "Write : Write\n",
            "a : a\n",
            "program : program\n",
            "to : to\n",
            "differentiate : differentiate\n",
            "stemming : stemming\n",
            "and : and\n",
            "lemmatizing : lemmatizing\n",
            "words : word\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wordnet Lemmatization and POS Tagging"
      ],
      "metadata": {
        "id": "mtiPaNiwQ5T7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from collections import defaultdict\n",
        "\n",
        "# Dictionary is created where pos_tag (first letter) are the key values whose values are mapped with the value from wordnet dictionary.\n",
        "# We have taken the only first letter as we will use it later in the loop.\n",
        "tag_map = defaultdict(lambda : wn.NOUN)\n",
        "tag_map['J'] = wn.ADJ\n",
        "tag_map['V'] = wn.VERB\n",
        "tag_map['R'] = wn.ADV\n",
        "\n",
        "text = \"Write a program to differentiate stemming and lemmatizing words\"\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "lemma_function = WordNetLemmatizer()\n",
        "\n",
        "# Loop is run and lemmatize will take two arguments one is token and other is a mapping of pos_tag with wordnet value.\n",
        "for token, tag in pos_tag(tokens):\n",
        "\t\tlemma = lemma_function.lemmatize(token, tag_map[tag[0]])\n",
        "\t\tprint(token, \"=>\", lemma)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-ebqQgJK4mC",
        "outputId": "1e5f7961-e9cd-4446-9e68-24e938597440"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write => Write\n",
            "a => a\n",
            "program => program\n",
            "to => to\n",
            "differentiate => differentiate\n",
            "stemming => stemming\n",
            "and => and\n",
            "lemmatizing => lemmatizing\n",
            "words => word\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W0kAcegCRjBt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}